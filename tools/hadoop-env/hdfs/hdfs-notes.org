* Hdfs notes
** Access remote HA cluster
Say you have got an EMR cluster, you need to access another remote HA cluster from spark on session level. Then you may need set the cluster global =hdfs-site.xml= like this:

 #+BEGIN_SRC xml
<configuration>
        <property>
                <name>dfs.nameservices</name>
                <value>cluster-default, cluster-remote</value>
        </property>
        <property>
                <name>dfs.ha.namenodes.cluster-remote</name>
                <value>nn1,nn2</value>
        </property>
        <property>
                <name>dfs.namenode.rpc-address.cluster-remote.nn1</name>
                <value>namenode1:8020</value>
        </property>
        <property>
                <name>dfs.namenode.rpc-address.cluster-remote.nn2</name>
                <value>namenode2:8020</value>
        </property>
        <property>
                <name>dfs.namenode.http-address.cluster-remote.nn1</name>
                <value>namenode1:50070</value>
        </property>
        <property>
                <name>dfs.namenode.http-address.cluster-remote.nn2</name>
                <value>namenode2:50070</value>
        </property>
        <property>
                <name>dfs.client.failover.proxy.provider.cluster-remote</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>
</configuration>
 #+END_SRC
 Then you need a new =core-site.xml= in your project which is referenced by spark like this:
 #+BEGIN_SRC python
        spark_session = spark_session.newSession()
        Path = spark_session._jvm.org.apache.hadoop.fs.Path
        core_path = Path(PROJECT_PATH + "/hive/hdfs/conf/core-site.xml")
        spark_session._jsc.hadoopConfiguration().addResource(core_path)

 #+END_SRC
 The new =core-site.xml= should look like this:
 #+BEGIN_SRC xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://cluster-remote</value>
  </property>
</configuration>

 #+END_SRC

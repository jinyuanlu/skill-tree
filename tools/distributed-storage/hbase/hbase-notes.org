** HBase Snapshots
- Snapshot allow you to take a copy of table with a very small performance effect.
- Snapshot is a immutable collection of hfile and metadata.
- Both "clone" and "restore" do not require any data to be copied cause the underlying Hfiles stay unchanged.
- "export" snapshot to remote cluster has little impact on RegionServer of the local clutster.
** Access to remote hbase snapshot from current EMR Hadoop cluster
Say you want to do this inside spark session.
*** create external table
#+BEGIN_SRC python
def create_table(self, hbase_table_name: str, hive_table_name: str):
    sql = self._get_sql(hbase_table_name, hive_table_name)

    command = """
    sql="{sql}"
    echo -e $sql | env HIVE_OPTS="-hiveconf hbase.zookeeper.quorum={quorum}" hive
    """.format(sql=sql, quorum=self.zookeeper_quorum)
    subprocess.run(command, shell=True, check=True)

#+END_SRC
*** create hbase snapshot
#+BEGIN_SRC python
HBASE_SHELL_CMD = "hbase --config {config_file_path} shell -n".format(config_file_path=PROJECT_PATH + "/hive/shell_config/")
def create_snapshot(self, hbase_table_name: str, hive_table_name: str):
    command = """
    echo "snapshot '{hbase_table_name}', '{hive_table_name}', {{SKIP_FLUSH => false}}" | {hbase_shell_cmd}
    """.format(hbase_table_name=hbase_table_name, hive_table_name=hive_table_name, hbase_shell_cmd=self.HBASE_SHELL_CMD)

    subprocess.run(command, shell=True, check=True)
#+END_SRC
*** config spark session
#+BEGIN_SRC python
    spark_session.sql("set hbase.zookeeper.quorum={quorum}".format(quorum=self.zookeeper_quorum))
    spark_session.sql("set hbase.rootdir=/hbase")
    spark_session.sql("set hive.hbase.snapshot.name={hive_table_name}".format(hive_table_name=hive_table_name))
    spark_session.sql("set hive.hbase.snapshot.restoredir=/tmp/hbase_snapshots/{hive_table_name}"
                        .format(hive_table_name=hive_table_name))

#+END_SRC
*** do somethine
#+BEGIN_SRC python
df: DataFrame = spark_session.sql("select * from {table_name}".format(table_name=hive_table_name)) \
                             .repartition(200)

#+END_SRC
*** drop snapshot
#+BEGIN_SRC python
def delete_snapshot(self, hive_table_name: str):
    command = """
    echo "delete_snapshot '{hive_table_name}'" | {hbase_shell_cmd}
    """.format(hive_table_name=hive_table_name, hbase_shell_cmd=self.HBASE_SHELL_CMD)

    subprocess.run(command, shell=True, check=True)

def _delete_table(self, hive_table_name: str):
    sql = "drop table {hive_table_name};".format(hive_table_name=hive_table_name)

    command = """
    sql="{sql}"
    echo -e $sql | hive
    """.format(sql=sql)

    subprocess.run(command, shell=True, check=True)
#+END_SRC

** Alter COMPRESSION
- describe 'table_name'
- alter 'table_name',NAME => 'family_name', COMPRESSION => 'snappy'
- major_compact 'table_name'
** Coprocessor
** Monitor
*** HMaster
**** Server Response Time
- HBase_Master_IPC_TotalCallTime_99th_percentile
**** GC
- HBase_JvmMetrics_GcCount
- HBase_JvmMetrics_GcTimeMillis
*** HRegionServer
**** RPC
- HBase_RegionServer_IPC_TotalCallTime_99th_percentile
- HBase_RegionServer_IPC_ProcessCallTime_99
- HBase_RegionServer_IPC_QueueCallTime_99th_percentile
- HBase_RegionServer_IPC_numActiveWriteHandler
- HBase_RegionServer_IPC_numActiveScanHandler
- HBase_RegionServer_IPC_numCallsInGeneralQueue
- HBase_RegionServer_IPC_numOpenConnections
**** Request
- HBase_RegionServer_Server_ScanTime_99th_percentile
- HBase_RegionServer_IO_FsReadTime_99th_percentile

**** Memory
- HBase_JvmMetrics_GcTimeMillis
- HBase_JvmMetrics_GcCount
- HBase_RegionServer_Server_PauseTimeWithoutGc_99th_percentile
- HBase_RegionServer_Server_memStoreSize

**** High-level Operate
- HBase_RegionServer_Server_compactionQueueLength
- HBase_RegionServer_Server_flushQueueLength
- HBase_RegionServer_Server_storeFileSize
